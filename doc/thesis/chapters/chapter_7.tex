\chapter{Empirical Evaluation}\label{experiments}
This chapter reports on the experimental study conducted to assess the efficiency of \acrshort{dpbt}. This study aims at answering the research questions that emerged from the motivation of this work. 

The fundamental part of this work focuses on incremental enumeration \acrshort{bt} in \acrshort{bg}. 
In spite of this, there are other important areas for doing empirical analysis such as memory consumption, thread scheduling, and execution time.

Regarding that, we have asked ourselves the following research question that guides the empirical evaluation analysis, and we try to answer with the conducted experiments:
\begin{inparaenum}[\bf {\bf RQ}1\upshape)]
\label{res:bt:question}
    \item Does \acrshort{dpbt} generate incremental results regardless of the size of the graph?
    \item Does the type of query $Q$ impact on the execution of \acrshort{dpbt}?
    \item How effectively \acrshort{dpbt} implements a \emph{pay-as-you-go} model?
    \item Does \acrshort{dpbt} handle memory and threads efficiently?
\end{inparaenum}
  
\section{Experiments Configuration}
We have conducted different kinds of experiments to answer our \qref{res:bt:question} and verify the correctness of the implementation.
First, we have performed a \emph{Diefficiency Metric Analysis}~\cite{diefpaper} in order to assess the continuous behavior capabilities of the implemented algorithm (generation of incremental results). 
Then, we have performed a \emph{Benchmark Analysis} to identify how the behavior of \acrshort{dpbt} varies depending on the type of query command selected for the user.
Finally, we have executed a \textit{Performance Analysis} in which we have to gather profiling data from \acrfull{ghc} for one of the graphs, 
to measure how the program performs regarding multithreading and memory allocation.
In the following subsections, we detail the different aspects of the configuration such as hardware, \acrshort{hs} compilation flags, metrics, and benchmark to conduct these experiments.


\subsection{Benchmark}\label{data:set}
The experiments have been evaluated over the networks that composed the benchmark Konect Networks~\cite{konect}. 
Specifically, the networks used in the literature have been selected \cite{konect:2017:dbpedia-recordlabel,konect:2017:moreno_crime,konect:2017:opsahl-ucforum,konect:2017:wang-amazon}.

\begin{table}[H]
  \centering
  \begin{tabular}{|p{0.25\linewidth}|c|c|c|c|c|}
    \hline
   \textbf{Network} & \textbf{$|U|$} & \textbf{$|L|$} & \textbf{$|E|$} & \textbf{Wedges} & \textbf{\#\acrshort{bt}} \\
   \hline
   Dbpedia & 18422 & 168338 & 233286 & $1.45 \times 10^8$ & $3.62 \times 10^8$\\
   \hline
   Moreno Crime & 829 & 551 & 1476 & 4816 & 211\\
   \hline
   Opsahl UC Forum  & 899 & 522 & 33720 & 174069 & $2.2 \times 10^7$ \\
   \hline
   Wang Amazon & 26112 & 799 & 29062 & $3.4 \times 10^6$ & 110269\\
   \hline
  \end{tabular}
 \caption[Selected Networks of \acrlong{bg}]{This tables shows the different networks use in the experiments. We provide some metrics of the networks used in order to understand a little more about the topology of each \acrshort{bg}. In particular we are showing in the last column $2$ metrics that are important and could affect results which are number of wedges and bi-triangles}
 \label{table:exp:data-set}
 \end{table}
 
The criteria for selecting those networks have followed the idea of conducting the analysis on one of the big networks~\cite{konect:2017:dbpedia-recordlabel} used on the \acrshort{bt} counting work~\cite{btcount}.
The rest of the networks, from the same data source~\cite{konect}, have been selected randomly but taking into consideration different sizes and topologies.

\subsection{Metrics}\label{sub:metric}
We report the following metrics \begin{inparaenum}[\bf a\upshape)]
      \item {\bf $dief@t$}: a measurement for continuous behavior in the first $t$ time units of the results generated by \acrshort{dpbt}~\cite{diefpaper},
      \item \emph{Average Running Time}: The average of the total running time of $1000$ resamples using \texttt{criterion} tool~\cite{criterion},
      \item \emph{Total Running Time}: Total running time of one execution set over each experiment setup and benchmark,
      \item \emph{GHC productivity}: Measures the proportion of \acrfull{mut} execution time vs. \acrfull{gc} time,
      \item \emph{Distribution of Threads per Core}: Measures the amount of threads per processor on each time slot of execution and
      \item \emph{Distribution of Allocated Memory per Data Type}: Measure the amount of allocated memory per \acrshort{hs} Data Type.
  \end{inparaenum}
    
\subsection{Experiments Scenarios}\label{sub:exp:exp-data-setup}
An experiment scenario is a specific configuration of the \acrfull{qo} that we have defined in \dref{def:query:match}. 
That means, that for each network that we run an experiment we can have a different configuration of the \acrshort{qo} used, allowing us to obtained different results.

\begin{definition}[Incidence Level]\label{def:exp:incidence}
Lets $G$ be a \acrlong{bg}.
Lets $v$ be a vertex such that $v \in V$ of $G$.
Lets $e$ be an edge such that $e \in E$ of $G$.
A \emph{low, medium, or high incidence level} for vertices is defined as following:
\begin{inparaenum}
  \item[Low] A vertex $v$ has low incidence if its degree is less than $1\%$ of $|V|$,
  \item[Medium] A vertex $v$ has low incidence if its degree is between $1\%-25\%$ of $|V|$, and
  \item[High] A vertex $v$ has low incidence if its degree is more than $25\%$ of $|V|$
\end{inparaenum}
A \emph{low, medium, or high incidence level} for edges is defined as following:
\begin{inparaenum}
  \item[Low] A edge $e = (u, l)$ has low incidence if any of its vertices $u$ or $l$ has degree less than $1\%$ of $|V|$,
  \item[Medium] A edge $e = (u, l)$ has low incidence if any of its vertices $u$ or $l$ has degree between $1\%-25\%$ of $|V|$, and
  \item[High]  A edge $e = (u, l)$ has low incidence if any of its vertices $u$ or $l$ has degree more than $25\%$ of $|V|$
\end{inparaenum}
\end{definition}


Having the previous \dref{def:exp:incidence}, we define the following scenarios to conduct all the experiments regardless the network used.

\begin{table}[H]
  \centering
  \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Scenario ID} & \textbf{Name} & \textbf{Search by}\\
    \hline
    E-H & Edge High & edge with high incidence \\
    \hline
    E-L & Edge Low & edge with low incidence \\
    \hline
    E-M & Edge Medium & edge with medium incidence \\
    \hline
    VL-H & $l \in L$ High & vertex in lower layer with high incidence \\
    \hline
    VL-L & $l \in L$ Low & vertex in lower layer with low incidence \\
    \hline
    VL-M & $l \in L$ Medium & vertex in lower layer with medium incidence \\
    \hline
    VU-H & $u \in U$ High & vertex in upper layer with high incidence \\
    \hline
    VU-L & $u \in U$ Low & vertex in upper layer with low incidence \\
    \hline
    VU-M & $u \in U$ Medium & vertex in upper layer with medium incidence \\
    \hline
  \end{tabular}
  \caption[Experiment Data Setup for experiments]{In the first column of the table we assign an identifier to be reference in the rest of the section. $E$ and $V$ indicates if the \acrshort{qo} scenario contains edges or vertices query respectively. $VL$ or $VU$ indicates if those vertices belongs to $L$ (lower layer) or $U$ upper layer. After the $-$ symbol the letter $L,M,H$ indicates the incidence level defined in \dref{def:exp:incidence}}
  \label{table:exp:data-setup}
  \end{table}

\paragraph{Selection of values for \acrshort{qo}}\label{sub:exp:sel-vals} The selection of values, either vertices $V$ or edges $E$, has been done pseudo-randomly.
By \emph{peudo-random} we mean that given \dref{def:exp:incidence}, we follow the next steps: \begin{inparaenum}[\bf i\upshape)]
  \item Sort the vertices by its degree,
  \item Selects randomly with equal probability a vertex or edge depending on the scenario, from the subset of vertices or edges that fulfill the \dref{def:exp:incidence}.
  \item We run a sample experiments to check if that selections provides results or not. If it provides results, we keep that selection.
\end{inparaenum}
  
\subsection{Implementation}
\paragraph{Hardware Platform} All the experiments have been executed in the \emph{HPC Cluster at UPC}. The nodes architecture running in the cluster is $x86$ $64$ bits with a \textit{$24$-Core Intel(R) Xeon(R) CPU X5650} processor of $2.67$ GHz. 
Regarding memory, the allocated nodes have been requested from $40 GB$ up to $120 GB$ of RAM for the biggest \acrshort{dbpedia} graph. These machines also have $256\ KB$ of L2 cache memory, and $12\ MB$ of L3 cache.

\paragraph{Haskell Setup} The implementation uses \acrshort{ghc} $8.10.4$ plus the following set of \acrshort{hs} libraries:
\begin{inparaenum}[]
  \item \texttt{dyanmic-pipeline} $0.3.2.0$ \cite{dynamic-pipeline},
  \item \texttt{bytestring} $0.10.12.0$ \cite{bytestring},
  \item \texttt{containers} $0.6.2.1$ \cite{containers}, 
  \item\texttt{relude} $1.0.0.1$ \cite{relude}
  \item and\texttt{unagi-chan} $0.4.1.3$ \cite{unagi} 
\end{inparaenum}. The \texttt{relude} library is utilized because \texttt{Prelude} was disabled from the project with the language extension \texttt{NoImplicitPrelude} \cite{extensions}. 
We have compiled our program using \texttt{stack} version $2.5.1$ \cite{stack} with the following command and option flags\footnote{For more information about package.yaml or cabal file please check https://github.com/jproyo/upc-miri-tfm/tree/main/bt-graph-dp}:
\mintinline[fontsize=\small, breaklines]{bash}{stack build --ghc-options "-threaded -O3 -rtsopts -with-rtsopts=-N"}.
Flag \texttt{threaded} indicates \acrshort{ghc} to compile the program with thread support enable. \texttt{-O3} is the highest optimization level for the compiler.
Regarding \texttt{-with-rtsopts=-N}, it allows us to change dynamically on each runtime execution command, the number of processors and other execution flags that we will explain in \autoref{par:ex:param}. 

\paragraph{Optimal Execution Parameters}\label{par:ex:param} \acrshort{ghc} enables different flags parameters to speed up the execution. Unfortunately there is no recipe to tune those parameters in the correct way and each program needs to be analyze to take advantage of \acrshort{ghc} capabilities.\footnote{Execution parameters details are explained in \aref{apx:running:experiments}}
The most important parameters to be tune in our case are memory and number of processor. In the case of number of processors we have run all the experiments between 6 and 12 cores. The detail of each run can be found in \autoref{sub:exp:exp-def}.
Setting up memory allocation is not a straightforward task, because we have at our disposal the combination of two parameters. \texttt{-A} flag which indicates the allocation area for the garbage collector, which is fixed and never resize and \texttt{-H} which is the heap size. There is no program or algorithm to find the optimal values for this two flags combined, but there exist a tool \texttt{ghc-gc-tune} \cite{ghctune} 
which implements an heuristic algorithm that tries different setup until it finds an optimal combination.
We have run \acrshort{dpbt} with that tool obtaining the following result that can be appreciated in \autoref{fig:exp:opt-mem}

\begin{figure}[h!]
  \centering  
  \resizebox{1\textwidth}{!}{%
  \includegraphics{bt-graph-dp-time-gc-space}
  }
\caption[$\dpbt$ Finding Optimal Memory Setup]{This figure shows the results obtained after running \acrshort{dpbt} with \texttt{ghc-gc-tune} tool in order to find the optimal configuration for Memory allocation. $y$ axis shows the total execution time, $x$ axis is the \texttt{-A} configuration flag and $z$ axis is the \texttt{-H} configuration flag.}
\label{fig:exp:opt-mem}
\end{figure}

In \autoref{fig:exp:opt-mem} we can appreciate that the tool runs serval time the same program with different configurations on flags \texttt{-A} and \texttt{-H} until if finds a minimum. The dark blue shows the better performance where the curve find its minimum execution time. 
This indicates two possible optimal setup, either when \texttt{-A} is equal to \texttt{-H}, or either when \texttt{-A} is $\frac{1}{4}$ of the \texttt{-H}. For all our experiments we have selected the first option which \texttt{-A} is equal to \texttt{-H}.

\section{Experiments Results}\label{sec:exp:observed-results}\label{sub:exp:exp-def}
\subsection{E1: Diefficiency Metric Analysis}\label{sub:sec:exp-1} 
\paragraph{Goal} In this experiment, we asses the ability of \acrshort{dpbt} to generate results incrementally.
In order to do that, we use \acrfull{dm} Tool \emph{diefpy}~\cite{diefpy} which implements \emph{Diefficiency Metric}~\cite{diefpaper} measurement analysis.
This experiment allows us to answer research questions [R1] and [R3] defined in \qref{res:bt:question}. 

\paragraph{Procedure} We execute this experiment for each networks described in \autoref{data:set} and for each scenario described in \autoref{sub:exp:exp-data-setup}.
This experiment have been executed five times on each case until we found the proper vertex of edges in the selection described in \autoref{sub:exp:sel-vals}.
In the following \autoref{table:e1:def} we detail the different configurations that we run for this experiment.

\begin{table}[H]
  \centering
  \resizebox{1\textwidth}{!}{%
  \begin{tabular}{|p{0.25\linewidth}|c|c|c|c|}
    \hline
   \textbf{Network} & \textbf{Scenario ID} & \textbf{Exec Flags} & \textbf{Query}\\
   \hline
   \multirow{9}{*}{Dbpedia} & E-H & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-edge (921, 4)}\\
   & E-L & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-edge (383, 397)}\\
   & E-M & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-edge (540, 60)}\\
   & VL-H & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-vertex 9}\\
   & VL-L & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-vertex 809}\\
   & VL-M & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-vertex 511}\\
   & VU-H & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-vertex 921}\\
   & VU-L & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-vertex 93}\\
   & VU-M & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-vertex 540}\\
   \hline
   \multirow{9}{*}{Moreno Crime} & E-H & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-edge (413, 419)}\\
   & E-L & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-edge (361, 19)}\\
   & E-M & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-edge (531, 196)}\\
   & VL-H & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 95}\\
   & VL-L & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 187}\\
   & VL-M & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 97}\\
   & VU-H & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-vertex 2}\\
   & VU-L & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 793}\\
   & VU-M & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 533}\\
   \hline
   \multirow{9}{*}{Opsahl UC Forum} & E-H & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-edge (213, 33)}\\
   & E-L & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-edge (398, 10)}\\
   & E-M & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-edge (129, 171)}\\
   & VL-H & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 289}\\
   & VL-L & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 258}\\
   & VL-M & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 433}\\
   & VU-H & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-vertex 395}\\
   & VU-L & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 390}\\
   & VU-M & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 207}\\
   \hline
   \multirow{9}{*}{Wang Amazon} & E-H & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-edge (839, 9)}\\
   & E-L & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-edge (10987, 36)}\\
   & E-M & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-edge (19630, 84)}\\
   & VL-H & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 124}\\
   & VL-L & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 321}\\
   & VL-M & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 64}\\
   & VU-H & \texttt{+RTS -A5G -N8 -c -H5G -RTS} & \texttt{by-vertex 1727}\\
   & VU-L & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 9970}\\
   & VU-M & \texttt{+RTS -A5G -N6 -c -H5G -RTS} & \texttt{by-vertex 73}\\
   \hline
  \end{tabular}
  }
 \caption[E1 Procedure]{This tables shows all the different experiments setups that we have conducted for E1. Execution flags are the Runtime execution flags which needs to be set on the execution command as it is detailed in \autoref{apx:running:experiments}. On the last column we can see for each case the query command executed for \acrshort{bt} enumeration}
 \label{table:e1:def}
 \end{table}

 \paragraph{Observed Results}\label{sub:sec:res:e1}
 Regarding the results obtained by \acrshort{dm} tool, the data is presented in two different formats: the graphics that show 
 how the different results are enumerated in each point of the time, and a radial graphic that shows the tension between aspects of the time measurement.
 In these graphs, the time represents the number of nanoseconds elapsed to deliver that result from the moment the \acrshort{dpbt} finishes the execution of $\ac$ and it started executing $\ad$, so $\ad$ is able to start processing commands. 
 Therefore, time $0$ is equal to the start of $\ad$ execution.
 
 \begin{figure}[!htb]
   \centering
   \begin{minipage}{0.5\textwidth}
    \includegraphics[width=1\linewidth, height=0.2\textheight]{experiments/diepfy/dbpedia.png}
     \caption{\acrshort{dm} Results: \acrshort{dbpedia}}
     \label{fig:dief:dbpedia}
   \end{minipage}%
   \begin{minipage}{0.5\textwidth}
    \includegraphics[width=1\linewidth, height=0.2\textheight]{experiments/diepfy/moreno_crime.png}
     \caption{\acrshort{dm} Results: Moreno Crime}
     \label{fig:dief:moreno}
   \end{minipage}
 \end{figure}
 %
 \begin{figure}[!htb]
   \centering
   \begin{minipage}{0.5\textwidth}
    \includegraphics[width=1\linewidth, height=0.2\textheight]{experiments/diepfy/opsahl-ucforum.png}
     \caption{\acrshort{dm} Results: Opsahl UC Forum}
     \label{fig:dief:opsahl}
   \end{minipage}%
   \begin{minipage}{0.5\textwidth}
     \includegraphics[width=1\linewidth, height=0.2\textheight]{experiments/diepfy/wang-amazon.png}
      \caption{\acrshort{dm} Results: Wang Amazon}
      \label{fig:dief:wang}
    \end{minipage}
  \end{figure}
 
 As observed in the results above in \autoref{fig:dief:dbpedia}, \autoref{fig:dief:moreno}, \autoref{fig:dief:opsahl} and \autoref{fig:dief:wang} in all the networks and experiments setups, \acrshort{bt} are incrementally enumerated and deliver to the user. 
 In all the cases, we observed how the results obtained in the experiment \emph{Vertex Lower with High Incidence}(VL-H) (see \autoref{table:exp:data-setup}), are more continuous compare to the rest of the experiments setups. 
 The behavior of this case can be explained because bi-triangles are aggregated based on some triple $\ell = (l_l,l_m,l_u)$ (see \dref{def:abt}) if the requested $l \in L$ matches with some of these triple elements, all the $\hat{U}_l$ cartesian product need to be enumerated in a 6-cycle path \acrshort{bt} (see \dref{def:bt}). Therefore, there are many \acrshort{bt} as a result of combining all $\hat{U}_l$ with $\ell$.
 
 The only experiment setup that cannot be appreciated with the same level of continuous results as the rest is \emph{Opsahl UC Forum} (see \autoref{fig:dief:opsahl}). 
 That is explained because the selection of the command $Q$ value (vertex or edge) for the different experiments scenario is \emph{pseudo-random} as it is explained in \autoref{sub:exp:sel-vals}. 
 We have detected that for this particular experiment setup \emph{Vertex Lower with High Incidence}(VL-H) and for network \emph{Opsahl UC Forum}, even when the \emph{pseudo-randomly} chosen vertex belongs to the Lower Layer $L$, it is not a vertex with a high incidence as it should be.
 In fact, it can be seen that for \emph{Vertex Upper with High Incidence}(VU-H) scenario in the same network, the \emph{pseudo-random} selection is a vertex that has high incidence which matches with the desired semantic of the scenario. 
 Unfortunatelly, because of the speed of the retrieval, incremental results cannot be appreciated properly\footnote{Check that the y-axis in the case of opsahl-ucforum and \acrshort{dbpedia} is $10^6$ scale as it is indicated on top of the graphic}. 
 
 The case of Moreno Crime in \autoref{fig:dief:moreno} seems the least continuous of all the experiments. 
 It could be explained by the fact of the topology of the graph, which is the smallest of all the graphs used, both in terms of the number of \acrshort{bt} and the number of wedges.
 Therefore, results are delivery extremely fast and incremental results only can be appreciated in the vertices with high incidence.
 
 \begin{figure}[!htb]
   \centering
   \begin{minipage}{0.5\textwidth}
    \includegraphics[width=1\linewidth, height=0.3\textheight]{experiments/diepfy/dbpedia_radial.png}
     \caption{\acrshort{dm} Results (Radial): \acrshort{dbpedia}}
     \label{fig:dief:dbpedia-radial}
   \end{minipage}%
   \begin{minipage}{0.5\textwidth}
    \includegraphics[width=1\linewidth, height=0.3\textheight]{experiments/diepfy/moreno_crime_radial.png}
     \caption{\acrshort{dm} Results (Radial): Moreno Crime}
     \label{fig:dief:moreno-radial}
   \end{minipage}
 \end{figure}
 %
 \begin{figure}[!htb]
   \centering
   \begin{minipage}{0.5\textwidth}
    \includegraphics[width=1\linewidth, height=0.3\textheight]{experiments/diepfy/opsahl-ucforum_radial.png}
     \caption{\acrshort{dm} Results (Radial): Opsahl UC Forum}
     \label{fig:dief:opsahl-radial}
   \end{minipage}%
   \begin{minipage}{0.5\textwidth}
     \includegraphics[width=1\linewidth, height=0.3\textheight]{experiments/diepfy/wang-amazon_radial.png}
      \caption{\acrshort{dm} Results (Radial): Wang Amazon}
      \label{fig:dief:wang-radial}
    \end{minipage}
  \end{figure}
 
 These radial plots obtained from \acrshort{dm} shows the tension between \acrfull{dt} (higher better),
 \acrfull{et}, \acrfull{tfft}, \acrfull{comp}, and finally, \acrfull{tt}. A perfect cover of each radial area with all the dimensions would
 indicate a perfect solution with incremental delivery of results, completeness, throughput, etc. The important part to remark here that verifies
 our assumption from the other graphics is that all the VL-H tension over \acrshort{dt} metrics are indicating that they are continuous on that part. 
 The rest of the data setup experiments indicates that the level of throughput, completeness, and execution time is less than \acrfull{dt}, and the results can be delivered faster outlying incremental behavior. 
 We know from the other graphics that is not true, since there are more experiments setups that also deliver incremental results throughout time.
 
 \paragraph{Conclusion E1} In conclusion, we are able to answer [R1], and asses that we have built an incremental algorithm for enumerating \acrlong{bt}. 
 The same conclusion can be obtained regarding question [R3]. We verify that depending on the incidence of the vertex or edge we are enumerating more or less \acrshort{bt} in a continuous manner. This shows us \acrshort{dpbt} effectively implements a \emph{pay-as-you-go} model.
 

\subsection{E2: Benchmark Analysis}\label{sub:sec:exp-2} 
\paragraph{Goal} Regarding benchmarking, we want to answer research question [R2] to know what is the impact of the type of command query $Q$ on \acrshort{bt} enumeration.
This will also indirectly answer [R3] because if a difference in the results based on the command is detected, we can answer [R3]. The experiment was conducted using the same networks and 
experiment setup as in \autoref{sub:sec:exp-1}, but removing \emph{\acrshort{dbpedia}} network and using \mintinline{shell}{criterion} \cite{criterion} benchmark tool.

\paragraph{Observed Results}\label{sub:sec:res:e2}
In the definition of \mintinline{shell}{criterion} tool \cite{criterion}, it can be found that each benchmark is conducting running the same experiments many times (by default 100 each),
doing a statistical analysis on the execution time, eliminating outliers, and fitting a regression model that could explain that behavior. 
In this experiment, we have conducted the benchmark using all the networks and the same experiment setup presented in \autoref{data:set}. 
We have not considered for the benchmark analysis \acrshort{dbpedia} because of its size and the time taken to finish a hundred of runs.

\begin{figure}[!htb]
  \begin{center}
     \includegraphics[width=1\textwidth] {experiments/bench_1}
       \end{center}
     \caption{Benchmark Results: Criterion Plot}
     \label{fig:exp:bench}
 \end{figure}

As it can be seen in \autoref{fig:exp:bench}, yellow bars are all the experiments related to Lower Layer vertices, blue bars are related to Upper Layer, and red bars are the experiments related to edge search. 
The longest bars are from network opsahl-ucforum, which we already know by \autoref{table:exp:data-set}, it is the biggest of the four networks in terms of the number of \acrshort{bt} and wedges. So, it needs to enumerate more \acrshort{bt}. 
On the other hand, almost all the high incidence vertices and edges queries are taken longer as well. 
There is only one outlier which is E-H (see \autoref{table:exp:data-setup}) for opsahl-ucforum, which is explained by the \emph{pseudo-randomly} selection detailed in \autoref{sub:exp:sel-vals}.

As we have explained, the tool fits a linear regression model with the observed empirical execution time.
We are not exposing the details of all these data for the regression model here, but the table can be found in \autoref{app:exp:bench}.
We can see from there, that there is only one case where the model could not fit the empirical data, which is VU-L for opsahl-ucforum with an $R^2$ of $0.718$.
This could be explained by the execution time variance, with a lower bound of $600$ms, an average of $2.43$s, and an upper bound of $3.18$s. 
This means that for some reason, this experiment was not stable or predictable from the execution time point of view.

Another benchmark we have measured was the total execution time of each experiment setup.

\begin{figure}[!htb]
  \begin{center}
     \includegraphics[width=1\textwidth] {experiments/execution_time_by_experiments}
       \end{center}
     \caption{Execution time results: Comparison between all setups}
     \label{fig:exp:bench:2}
 \end{figure}

Regarding \autoref{fig:exp:bench:2}, we have already seen in the other experiments that \acrshort{dbpedia} was the one which took more time in all experiments setups.
This is perfectly explained by the characteristics and topology of the graph. We can also see in this image that the experiments with higher incidence take more time to finalize the execution compared to the experiments with lower incidence. 

\paragraph{Conclusion E2} We can answer the question [R2] because it can clearly be seen in the benchmark analysis and in \autoref{fig:exp:bench:2} that as long as the user request for command queries $Q$ that have more incidence in the graph and participates in more \acrshort{bt},
the execution time increases.

\subsection{E3: Performance Analysis}\label{sub:sec:exp-3} 
\paragraph{Goal} In this experiment, we take measurements on one network, to take measurements about the use of memory and threads on \acrshort{ghc}, using
\mintinline{shell}{ThreadScope} \cite{threadscope} and \mintinline{shell}{eventlog2html} \cite{eventlog2html} tools. 

\paragraph{Observed Results}\label{sub:sec:res:e3}
We can divide the analysis of this section into two: memory consumption and multithreading.

\paragraph{Multithreading} Regarding multithreading, we have gathered multithreading metrics of different time slots of the total excution time of Moreno Crime network run.\footnote{We could not analyze bigger networks due to the huge amount of data gathered that make the program timeout (24 hours) running out of memory}.
As we can see in the overview, execution in \autoref{fig:exp:perf:1} all the cores (8) are running \acrfull{mut} time in threads almost during the whole execution of the program.
\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
     \includegraphics[width=0.48\textwidth] {experiments/thread/general_overview}
       \end{center}
     \caption{Thread Metrics: General overview}
     \label{fig:exp:perf:1}
 \end{wrapfigure}
This indicates that there are few GC pauses, and running time is overtaken by \acrshort{mut} time and not GC. In fact, \acrshort{ghc} productivity on this run indicated $99.8\%$.
ThreadScope~\cite{threadscope} output allows us to zoom in different portions of the execution time to analyze the results better. 
If we zoom in on the execution threads at the beginning and at the end, we are going to see that there is a moment when only one core is executing. 
That fits perfectly with the model since at the beginning \acrshort{dp} setups all the filters and starts reading the input file. 
Remember that each stage runs on its own thread. In the end, also it is explained by the \acrshort{dp} paradigm because it happens the same as, $\ibt$ but in $\obt$.
In the middle of the execution, where there is more processing of the $\fbt$, we can see that the threads are distributed evenly between the cores and the same 
the behavior appears regarding less use of GC and higher \acrshort{mut} time.

\begin{figure}[!htb]
  \centering
  \begin{minipage}{0.3\textwidth}
   \includegraphics[width=1\linewidth, height=0.2\textheight]{experiments/thread/init}
   \caption{Thread Metrics: Initial execution}
   \label{fig:exp:perf:2}
  \end{minipage}%
  \hspace{.3cm}%
  \begin{minipage}{0.3\textwidth}
    \includegraphics[width=1\linewidth, height=0.2\textheight]{experiments/thread/middle}
    \caption{Thread Metrics: Middle of execution}
    \label{fig:exp:perf:4}
   \end{minipage}% 
   \hspace{.3cm}%
  \begin{minipage}{0.3\textwidth}
   \includegraphics[width=1\linewidth, height=0.2\textheight]{experiments/thread/end}
   \caption{Thread Metrics: End of execution}
   \label{fig:exp:perf:3}
  \end{minipage}%
\end{figure}

\paragraph{Memory Consumption} In the case of memory consumption, we have been able to measure the memory consumption for the biggest graph, \acrshort{dbpedia}. 
As it is known, enabling profiling downgrades performance of execution time. Because of that, the program runs out of memory as we are going to see in the image. Although this, we have still been able to gather interesting data to analyze memory allocation.

\begin{figure}[!htb]
  \begin{center}
     \includegraphics[width=1\textwidth] {experiments/mem/overview}
       \end{center}
     \caption{Memory Metrics: Allocation by Data Type}
     \label{fig:exp:mem:1}
 \end{figure}

As we can appreciate in \autoref{fig:exp:mem:1} the darkest blue space belongs to \mintinline{shell}{MUT_ARR_PTRS_CLEAN}.
This type of objects are pointers to function. In \acrlong{hs} \acrshort{mut} is the acronym of a thread evaluating an \acrshort{hs} expression.
That means that there are many pointers allocated waiting for evaluating expressions. This is perfectly explained by the \acrshort{dp} model, in which we are spawning one
thread per stage, and in particular, that means one thread per filter instance as well. In the case of \acrshort{dbpedia} which contains $168.338$ vertices in 
$L$ according to \autoref{table:exp:data-set}, we are spawning the same amount of thread for every run of this network. Since the execution of all these stages will not be released until it finishes the last $\ad$ (processing queries), all of them are waiting for the queries to be processed and executed.
The rest of the memory that is shown in the image is alive data objects such as \mintinline{haskell}{Maybe} and state of the filters instance \mintinline{haskell}{IntSet}. This data is less than $30\%$ of the total allocated memory. The linear growth of the memory could be explained by the reasons exposed before. 
All the $\fbt$ instances are created as long the program executes.
One of the proposed solutions for future work is to reduce the number of $\fbt$ for bigger graphs in order to reduce the number of allocated pointers waiting for commands.

\paragraph{Conclusion E3} We can answer the question [R4] as we have shown that threads are efficiently handled by \acrshort{hs} \acrshort{ghc} scheduler supporting the parallelization level that \acrshort{dp} requires. 
On the other hand, for completely answering the research question regarding memory consumption, we need to take into consideration the size of the graph.
Because of the algorithm proposed for solving the enumeration problem, there is a linear increase of the complexity on the size of the graph. In spite of this, we believe that this aspect can be improved by reducing the number of $\fbt$ instances. Moreover, choosing suitable data structures used for searching and deliver \acrshort{bt} are going to improve the execution time as well. 
It was out of the scope of this work to solve the efficiency of the queries, as well as the underlying data structures improvements. 

\section{Chapter Summary}
In this chapter, we have explained in extended detail all the experiments conducted in order to answer the research question exposed.
We first started doing a summary of the research questions. After that, we have presented the setup of the experiments, data, and execution environment details.
Then we have described the experiments, the results, and we have discussed them in deep. At the end of each discussion, we have given a brief conclusion about the observed results and the suitability for answering the pertinent research questions.
