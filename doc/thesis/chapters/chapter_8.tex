\chapter{Conclusions and Future Work}\label{conclusions}
In this chapter, we present the future work that we think it is interesting to address as a result of this research, 
as well as the conclusions obtained.

\section{Future Work}
One of the first things that have been out of the scope of this work and it will be tremendously important to address in order to be able to process huge networks efficiently is 
the selection of better data structures for handling the search in the compress \acrshort{bt} once the query command arrives at the $\ad$.
We have seen in \autoref{sub:sec:res:e3} how there is a memory bottleneck that is not allowing the execution of bigger graphs without paying so much extra cost. 
Performing indexing over the edges and vertices could lead to important improvements in search, although there is a trade-off in terms of memory allocation.
On the other hand, and continuing with the same idea, an important work to be addressed in the future is the distribution of the stages not only between threads but also between machines. 
We believe that moving from a multithreading model to a distributed model will allow \acrlong{dp} to reduce the gap of memory consumption for bigger graphs instances, as well as distribute the stage according to the amount of memory required by them. 
This leads to a similar scenario as before, there is a trade-off in terms of transfer data and the delay that this kind of distributed computation brings, but 
since we are delivering incremental results and we have implemented a \emph{pay-as-you-go} model, the distribution cost could be amortized; without mentioning that the speed up and reliance on network communication is higher than ever.

In another aspect of the computational model and more related with the \acrlong{hs} implementation, there are several improvements to be conducted there as well. One of them could be
to delegate the distribution to other stream processing systems like Kafka~\cite{kafka} for example, and do the parallel processing of the split data with \acrshort{hs} consuming from them.
Moreover, some radical improvements can be done into \acrshort{dpfh}. From designing more abstractions to help the user to write pipelines with less effort and errors, up to helping the thread schedule and memory management to perform even better than it is now. 
One of this speedup could be moving some types to be Unboxed types~\cite{hs-unbox} which reduce memory footprint.

\section{Conclusions}
Dealing with $NP$-complete problems like Enumerating \acrlong{bt} in \acrlong{bg}, is a task that, in general, is addresses using approximation algorithm techniques or heuristics. 
On the other hand, we have the capability of introducing algorithms in a \emph{pay-as-you-go} model approach combined with a model of computation that enables building incremental solutions like \acrshort{dp} which are exhaustive and exact for the solution space that the user wants.    
\acrfull{dp} has shown to be a quite efficient model for building parallel algorithms for solving a complex problem like Enumeration of Bi-triangles in \acrlong{bg}.
We have shown how strong is this paradigm in companion with a powerful language like \acrfull{hs}. The capabilities of combining both not only for implementing the concepts but also for evaluating empirically. 
We have also shown that the designed algorithm \acrshort{dpbt} can process and enumerate big networks like the \acrlong{dbpedia} which contains more than $300$ millions of \acrlong{bt}.
In the experimental analysis, we have been able to demonstrate the capabilities of the implementation to deliver incremental results with all the benefits that this implies. 
In conclusion, we believe that the achieved results open a wide range of possibilities not only to improve the existing framework and algorithm but also to implement other complex problems where the use case requires a \emph{pay-as-you-go} model.
