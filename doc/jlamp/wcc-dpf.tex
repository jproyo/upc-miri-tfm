\section{Enumerating Weakly Connected Component on the DPF}\label{sec:wcc-dpf}

Having the \acrshort{dpfh} in place as we describe in the previous \autoref{dp-hs}, we implement the algorithm for enumerating weakly components of a graph using the \acrshort{dpfh}.
We describe first the general algorithm in the context of \acrshort{dp}.
Secondly, we show the implementation details of that algorithm in the context of the \acrshort{dpfh} and finally we show the experiments and results conducted over that implementation.

\iffalse
\subsection{\texorpdfstring{$\dpwcc$}{Lg} Algorithm}\label{sub:sec:wcc:algo}
Let us consider the problem of computing/enumerating the (weak) connected components of a graph $G$ using \acrshort{dp}. 
A connected component of a graph is a subgraph in which any two vertices are connected by paths.  
Thus, finding connected components of an undirected graph implies obtaining the minimal partition of the set of nodes induced by the relationship \textit{connected}, i.e., there is a path between each pair of nodes. 
An example of that graph can be seen in \autoref{fig:example_dp_graph}.
The input of the Dynamic Pipeline for computing the WCC of a graph, $\dpwcc$, is a sequence of edges ending with $\eof$\footnote{Note that there are neither isolated vertices nor loops in the source graph $G$.}. 
The connected components are output as soon as they are computed, i.e., they are produced incrementally. 
Roughly speaking the idea of the algorithm is that the weakly connected components are built in two phases. 
In the first phase filter instance stages receive the edges of the input graph and create sets of connected vertices. 
During the second phase, these filter instances construct maximal subsets of connected vertices, i.e. the vertices corresponding to (weakly) connected components.
%
$\dpwcc$ is defined in terms of the behavior of its four kinds stages: \textit{Source} ($\iwc$),  \textit{Generator} ($\gwc$),  \textit{Sink} ($\owc$), and \textit{Filter}($\fwc$) stages. Additionally,  the channels connecting these stages must be defined. 
In $\dpwcc$, stages are connected linearly and unidirectionally through the channels $\ice$ and  $\csofv$. Channel $\ice$ carries edges while channel  $\csofv$ conveys sets of connected vertices. Both channels end by the $\eof$ mark. 
The behavior of $\fwc$ is given by a sequence of two actors (scripts). Each actor corresponds to a phase of the algorithm. In what follows, we denote these actors by $\Act$ and $\Actt$, respectively. 
The script $\Act$ keeps a set of connected vertices ($CV$) in the state of the $\fwc$ instance. When an edge $e$ arrives, if an endpoint of $e$ is present in the state, then the other endpoint of $e$ is added to $CV$. 
Edges without incident endpoints are passed to the next stage. When $\eof$ arrives at channel $\ice$, it is passed to the next stage, and the script $\Actt$ starts its execution. 
If script $\Actt$ receives a set of connected vertices $CV$ in $\csofv$, it determines if the intersection between $CV$ and the nodes in its state is not empty. If so, it adds the nodes in $CV$  to its state. 
Otherwise, the $CV$ is passed to the next stage. Whenever $\eof$ is received, $\Actt$ passes--through $\csofv$-- the set of vertices in its state and the $\eof$ mark to the next stage; then, it dies.
The behavior of $\iwc$ corresponds to the identity transformation over the data stream of edges.  As edges arrive, they are passed through  $\ice$ to the next stage. When receiving $\eof$ on $\ice$, this mark is put on both channels. 
Then, $\iwc$ dies. 

%\begin{wrapfigure}{r}{0.4\textwidth}
\begin{figure}
 \begin{center}
\inputtikz{graph_example_wcc}
\end{center}
\caption[{[PoC] Graph WCC Example}]{Example of a graph with two weakly connected components: $\{1,2\}$ and $\{3,4,5,6\}$}
\label{fig:example_dp_graph}
\end{figure}
%\end{wrapfigure}

Let us describe this behavior with the example of the graph shown in \autoref{fig:example_dp_graph}.

\begin{figure}[h!]
  \centering
\inputtikz{dp_example_0}
\caption[{[PoC] $\dpwcc$ Initial Setup}]{$\dpwcc$ Initial setup. Stages Source, Generator, and Sink are represented by the squares labeled by $\mathsf{Sr_{WCC}}$, $\mathsf{G_{WCC}}$ and $\mathsf{Sk_{WCC}}$, respectively.  The square $\fwc$ corresponding to the Filter stage template is the parameter of $\gwc$. Arrows $\rightrightarrows$ between represents the connection of stages through two channels, $\ice$, and $\csofv$. The arrow  $\rightarrow$ represents the channel $\csofv$ connecting the stages $\mathsf{G_{WCC}}$ and $\mathsf{Sk_{WCC}}$. The arrow $\Longrightarrow$ stands for I/O data flow. Finally, the input stream comes between the dotted lines on the left and the WCC computed incrementally will be placed between the solid lines on the right.}
\label{fig:dp_example_0}
\end{figure}

\autoref{fig:dp_example_0} depicts the initial configuration of $\dpwcc$. 
The interaction of $\dpwcc$ with the "external" world is done through the stages $\iwc$ and $\owc$. 
Indeed, once activated the initial $\dpwcc$, the input stream -- consisting of a sequence containing all the edges in the graph in \autoref{fig:example_dp_graph} -- feeds $\iwc$ while  $\owc$ emits incrementally the resulting weakly connected components.  
In what follows \autoref{fig:dp_example_1_2}, \autoref{fig:dp_example_3_4}, \autoref{fig:dp_example_5_6}, \autoref{fig:dp_example_7_8} and \autoref{fig:dp_example_9_10} depict the evolution of the $\dpwcc$.
 
\begin{figure}[h!]
\centering
\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_1}
  \caption{The edge $(1,2)$ is arriving to $\gwc$.}
  \label{fig:dp_example_1_2a}
\end{subfigure}
\vspace{.3cm}

\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_2}
  \caption{When the edge $(1,2)$ arrives to $\gwc$, it  spawns a new instance of $\fwc$ before $\gwc$. Filter instance $F_{\{1,2\}}$ is connected to  $\gwc$ through channels $\ice$ and  $\csofv$. The state of the new filter instance $F_{\{1,2\}}$ is initialized with the set of vertices $\{1,2\}$. The edge $(3,6)$ arrives to the new filter instance $F_{\{1,2\}}$.}
  \label{fig:dp_example_1_2b}
\end{subfigure}
\caption[{[PoC] $\dpwcc$ Evolving first state}]{Evolution of the $\dpwcc$: First state}
\label{fig:dp_example_1_2}
\end{figure}
\vspace{.5cm}

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_3}
  \caption{None of the vertices in the edge $(3,6)$ is in the set of vertices $\{1,2\}$ in the state of $F_{\{1,2\}}$, hence it is passed through $\ice$ to $\gwc$.}
  \label{fig:dp_example_3_4a}
\end{subfigure}
\vspace{.3cm}

\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_4}
  \caption{When the edge $(3,6)$ arrives to $\gwc$, it spawns the filter instance $F_{\{3,6\}}$  between $F_{\{1,2\}}$ and $\gwc$. Filter instance $F_{\{1,2\}}$ is connected to the new filter instance $F_{\{3,6\}}$ and this one is connected to  $\gwc$ through channels $\ice$ and  $\csofv$. The state of the new filter instance $F_{\{3,6\}}$ is initialized with the set of vertices $\{3,6\}$. The edge $(3,4)$ arrives to $F_{\{1,2\}}$  and $\mathsf{Sr_{WCC}}$ is fed with the mark $\eof$. Edges $(3,4)$ and $(4,5)$ remain passing through $\ice$.}
  \label{fig:dp_example_3_4b}
\end{subfigure}
\caption[{[PoC] $\dpwcc$ Evolving second state}]{Evolution of the $\dpwcc$: Second state}
\label{fig:dp_example_3_4}
\end{figure}
\vspace{.5cm}

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_5}
  \caption{$\mathsf{Sr_{WCC}}$  fed both, $\ice$ and $\csofv$, channels with the mark $\eof$ received from the input stream in previous state and then, it died. The edge $(4,5)$ is arriving to $\gwc$ and the edge $(3,4)$ is arriving to $F_{\{3,6\}}$. }
  \label{fig:dp_example_5_6a}
\end{subfigure}
\vspace{.3cm}

\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_6}
  \caption{When the edge $(4,5)$ arrives to $\gwc$, it spawns the filter instance $F_{\{4,5\}}$  between $F_{\{3,6\}}$ and $\gwc$. Filter instance $F_{\{3,6\}}$ is connected to the new filter instance $F_{\{4,5\}}$ and this one is connected to  $\gwc$ through channels $\ice$ and  $\csofv$.  Since the edge $(3,4)$ arrived to $F_{\{3,6\}}$ at the same time and  vertex $3$ belongs to the set of connected vertices of the filter $F_{\{3,6\}}$,  the vertex $4$ is added to the state of $F_{\{3,6\}}$. Now, the state of $F_{\{3,6\}}$ is the connected set of vertices $\{3,4,6\}$. When the mark $\eof$ arrives to the first filter instance, $F_{\{1,2\}}$, through  $\csofv$, this stage passes  its partial set of connected vertices,  $\{1,2\}$, through $\csofv$ and dies.  This action will activate $\Actt$ in next  filter instances to start building  maximal connected components. In this example, the state in  $F_{\{3,6\}}$, $\{3,4,6\}$, and the arriving set $\{1,2\}$ do not intersect and, hence, both sets of vertices, $\{1,2\}$ and $\{3,4,6\}$ will be passed  to the next filter instance through $\csofv$.}
  \label{fig:dp_example_5_6b}
\end{subfigure}
\caption[{[PoC] $\dpwcc$ Evolving third state}]{Evolution of the $\dpwcc$: Third state}
\label{fig:dp_example_5_6}
\end{figure}
\vspace{.5cm}

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_7}
  \caption{The set of connected vertices  $\{3,4,6\}$ is arriving to $F_{\{4,5\}}$. The mark $\eof$ continues passing to next stages through the channel $\ice$.}
  \label{fig:dp_example_7_8a}
\end{subfigure}
\vspace{.3cm}

\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_8}
  \caption{Since the intersection of the set of connected vertices $\{3,4,6\}$ arrived to  $F_{\{4,5\}}$ and its state is not empty, this state is enlarged to be $\{3,4,5,6\}$. The set of connected vertices $\{1,2\}$ is arriving to  $F_{\{4,5\}}$}
  \label{fig:dp_example_7_8b}
\end{subfigure}
\caption[{[PoC] $\dpwcc$ Evolving fourth state}]{Evolution of the $\dpwcc$:  Fourth state}
\label{fig:dp_example_7_8}
\end{figure}
\vspace{.5cm}

\begin{figure}[h!]
\centering
\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_9}
  \caption{$F_{\{4,5\}}$ has passed the set of connected vertices  $\{1,2\}$ and it is arriving to $\mathsf{Sk_{WCC}}$. The mark $\eof$ is arriving to  $F_{\{4,5\}}$ through $\csofv$.}
  \label{fig:dp_example_9_10a}
\end{subfigure}
\vspace{.3cm}

\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_10}
  \caption{Since the mark $\eof$ arrived to $F_{\{4,5\}}$ through $\csofv$, it passes its state, the set $\{3,4,5,6\}$ through $\csofv$ to next stages and died. The set of connected vertices  $\{1,2\}$ arrived to $\mathsf{Sk_{WCC}}$ and this implies  that $\{1,2\}$ is a maximal set of connected vertices, i.e. a connected component of the input graph. Hence,  $\mathsf{Sk_{WCC}}$ output this first weakly connected component.}
 \label{fig:dp_example_9_10b}
\end{subfigure}
\vspace{.5cm}

\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_11}
  \caption{Finally, the set of connected vertices  $\{3,4,5,6\}$ arrived to $\mathsf{Sk_{WCC}}$ and was output as a new weakly connected component. Besides, the mark $\eof$ also arrived to $\mathsf{Sk_{WCC}}$ through $\csofv$ and thus, it dies.}
  \label{fig:dp_example_9_10c}
\end{subfigure}
\vspace{.3cm}

\begin{subfigure}[b]{\textwidth}
 \centering
  \inputtikz{dp_example_12}
  \caption{The weakly connected component of in the graph \autoref{fig:example_dp_graph} such as they have been emitted by $\dpwcc$.}
  \label{fig:dp_example_9_10d}
\end{subfigure}
\caption[{[PoC] $\dpwcc$ Evolving last state}]{Last states in the evolution of the $\dpwcc$}
\label{fig:dp_example_9_10}
\end{figure}

It is importat to highlight that during the states shown in \autoref{fig:dp_example_1_2a},  \autoref{fig:dp_example_1_2b},  \autoref{fig:dp_example_3_4a},  \autoref{fig:dp_example_3_4b} and  \autoref{fig:dp_example_5_6a} the only actor executed in any filter instance is $\Act$ (constructing sets of connected vertices). Afterwards, although $\Act$ can continue being executed in some filter instances, there are some instances that start executing $\Actt$ (constructing sets of maximal connected vertices). This is shown from \autoref{fig:dp_example_5_6a}  to \autoref{fig:dp_example_9_10a}.
\fi
%
\clearpage
%
\subsection{\texorpdfstring{$\dpwcc$}{Lg} Implementation}
%
As we said before, the $\dpwcc$ implementation has been made as a proof of concept to understand and explore the limitations and challenges that we could find in the development of a future \acrshort{dpf} in \acrshort{hs}. 
In Section \ref{dp-hs} we emphasize that the focus of \acrshort{dpf} in \acrshort{hs} is on the \acrshort{idl} component. 
Hence, the development of the $\dpwcc$ is as general as possible using most of the constructs and abstractions required by the \acrshort{idl}. 
Lets introduce the minimal code needed for encoding any \acrshort{dp} using \acrshort{dpfh}. \footnote{All the code that we expose here can be accessed publicly in \url{https://github.com/jproyo/dynamic-pipeline/tree/main/examples/Graph}}

\begin{listing}[H]
  \begin{minted}[fontsize=\fontsize{10}{11}\selectfont,numbers=left,breaklines,frame=lines,framerule=2pt,framesep=2mm,baselinestretch=1.2,highlightlines={1-3,6}]{haskell}
    
    type DPConnComp = Source (Channel (Edge :<+> ConnectedComponents :<+> Eof))
                :=> Generator (Channel (Edge :<+> ConnectedComponents :<+> Eof))
                :=> Sink

    program :: FilePath -> IO ()
    program file = runDP $ mkDP @DPConnComp (source' file) generator' sink'
        
  \end{minted}
  \caption[{[\mintinline{shell}{ConnectedComp.hs}] Main entry point of the program}]{In this code we can appreciate the main construct of our $\dpwcc$ which is a combination of $\iwc$, $\gwc$ and $\owc$}
  \label{src:dpwcc:1}
\end{listing}

In \autoref{src:dpwcc:1} there are two important declarations. First, the \textit{Type Level} declaration of the $\dpwcc$ to indicate \acrshort{dpfh} how our stages are going be connected, and
using that \textit{Type Level} construct, we use the \acrshort{idl} to allow the framework interpret the type representation of our \acrshort{dp} and ensuring at compilation time that we provide the correct stages,  \textit{Source} ($\iwc$), \textit{Generator} ($\gwc$) and \textit{Sink} ($\owc$), that matches those declaration.
According to this declaration what we need to provide is the correct implementation of \mintinline{haskell}{source'}, \mintinline{haskell}{generator'} and \mintinline{haskell}{sink'}
which \textit{Type checked} the \acrshort{dp} type definition\footnote{The names of the functions are completely choosen by the user of the framework and it should not be confused with the internal framework combinators.}.

\begin{listing}[H]
  \begin{minted}[fontsize=\fontsize{10}{11}\selectfont,numbers=left,breaklines,frame=lines,framerule=2pt,framesep=2mm,baselinestretch=1.2,highlightlines={4-5,8}]{haskell}
    
    source' :: FilePath
            -> Stage
              (WriteChannel Edge -> WriteChannel ConnectedComponents -> DP st ())
    source' filePath = withSource @DPConnComp
      $ \edgeOut _ -> unfoldFile filePath edgeOut (toEdge . decodeUtf8)

    sink' :: Stage (ReadChannel Edge -> ReadChannel ConnectedComponents -> DP st ())
    sink' = withSink @DPConnComp $ \_ cc -> withDP $ foldM_ cc print

    generator' :: GeneratorStage DPConnComp ConnectedComponents Edge st
    generator' =
      let gen = withGenerator @DPConnComp genAction
      in  mkGenerator gen filterTemplate
        
  \end{minted}
  \caption[{[\mintinline{shell}{ConnectedComp.hs}] $\iwc$, $\gwc$ $\owc$ Code}]{In this code we can appreciate the $\iwc$, $\gwc$ and $\owc$ functions that matches the type level definition of the $\DP$. $\iwc$ and $\owc$ are completely trivial but $\gwc$ will be analyzed later due to its internal complexity.}
  \label{src:dpwcc:2}
\end{listing}

As we appreciate in \autoref{src:dpwcc:2}, $\iwc$ and $\owc$ are trivial. In the case of \mintinline{haskell}{source'} the only work it needs to do is to read the input data edge by edge and downstream to the next stages. 
Thats is a achieve with a \acrshort{dpfh} combinator called \mintinline{haskell}{unfoldFile} which is a catamorphism of the input data to the stream.
In the case of $\owc$ it is also a simple implementation but doing the opposite as $\iwc$ using an anamorphism combinator provided by the framework as well, which is \mintinline{haskell}{foldM_}.
The $\gwc$ Stage is a little more complex because it contains the core of the algorithm explained in \autoref{sub:sec:wcc:algo}. According to what we described in \autoref{sec:dp}, \textit{Generator} stage spawns a \textit{Filter} on each received edge in our case of $\dpwcc$.
Therefore, it needs to contain that recipe on how to generate a new \textit{Filter} instance -- in our case of \acrshort{hs} it is a defunctionalized Data Type or Function --. 
Then, we have two things \mintinline{haskell}{genAction} which tells how to spawn a new \textit{Filter} and under what circumstances, and \mintinline{haskell}{filterTemplate} with the function to be spawn.

\begin{listing}[H]
  \begin{minted}[fontsize=\fontsize{10}{11}\selectfont,numbers=left,breaklines,frame=lines,framerule=2pt,framesep=2mm,baselinestretch=1.2,highlightlines={8-10}]{haskell}
    
    genAction :: Filter DPConnComp ConnectedComponents Edge st
              -> ReadChannel Edge
              -> ReadChannel ConnectedComponents
              -> WriteChannel Edge
              -> WriteChannel ConnectedComponents
              -> DP st ()
    genAction filter' readEdge readCC _ writeCC = do
      let unfoldFilter = mkUnfoldFilterForAll filter' toConnectedComp readEdge (readCC .*. HNil) 
      results <- unfoldF unfoldFilter
      foldM_ (hHead results) (`push` writeCC)
        
  \end{minted}
  \caption[{[\mintinline{shell}{ConnectedComp.hs}] Generator Action Code}]{In this code we can appreciate the Generator Action code which will expand all the filters in runtime in front of it and downstream all the connected components calculated for those, to the Sink}
  \label{src:dpwcc:3}
\end{listing}

\acrshort{dpfh} provides several combinators to help the user with the \textit{Generator} code, in particular with the spawning process as it has been describe in \autoref{dp-hs}.
\mintinline{haskell}{genAction} for $\dpwcc$ will use the combinator \mintinline{haskell}{mkUnfoldFilterForAll} which will spawn one \textit{Filter} per received edge in the channel, expanding dynamically the stages on runtime.
In the third highlighted line we can appreciate how after expanding the filters, the generator will downstream to the \textit{Sink}, the received Connected Components calculated from previous filters.

\begin{listing}[H]
  \begin{minted}[fontsize=\fontsize{10}{11}\selectfont,numbers=left,breaklines,frame=lines,framerule=2pt,framesep=2mm,baselinestretch=1.2,highlightlines={2,11-15,24-31}]{haskell}
    
    filterTemplate :: Filter DPConnComp ConnectedComponents Edge st
    filterTemplate = actor actor1 |>> actor actor2
    
    actor1 :: Edge
           -> ReadChannel Edge
           -> ReadChannel ConnectedComponents
           -> WriteChannel Edge
           -> WriteChannel ConnectedComponents
           -> StateT ConnectedComponents (DP st) ()
    actor1 _ readEdge _ writeEdge _ = 
      foldM_ readEdge $ \e -> get >>= doActor e
     where
      doActor v conn
        | toConnectedComp v `intersect` conn = modify' (toConnectedComp v <>)
        | otherwise = push v writeEdge
    
    actor2 :: Edge
           -> ReadChannel Edge
           -> ReadChannel ConnectedComponents
           -> WriteChannel Edge
           -> WriteChannel ConnectedComponents
           -> StateT ConnectedComponents (DP st) ()
    actor2 _ _ readCC _ writeCC = do 
      foldWithM_ readCC pushMemory $ \e -> get >>= doActor e
    
     where
       pushMemory = get >>= flip push writeCC
    
       doActor cc conn
        | cc `intersect` conn = modify' (cc <>)
        | otherwise = push cc writeCC
    
  \end{minted}
  \caption[{[\mintinline{shell}{ConnectedComp.hs}] Filter Template Code}]{Filter template code composed by 2 Sequential Actors that will calculate the Connected Components and downstream them.}
  \label{src:dpwcc:4}
\end{listing}

Finally, in \autoref{src:dpwcc:4} the \textit{Filter} template code is defined. 
As we have seen in \autoref{sub:sec:wcc:algo}, $\dpwcc$ \textit{Filter} is composed of 2 Actors. The first actor collect all the possible vertices that are incidence some of the vertices edge that was instantiate with.
Once it does not receive any more edges, it starts downstream it set of vertices to the following filters in order to build a maximal connected components, that is \mintinline{haskell}{actor2}. At the end \mintinline{haskell}{actor2} will downstream 
its connected component to the following stages.
As we show, with the help of the \acrlong{dpfh}, building a \acrshort{dp} algorithm like \acrshort{wcc} enumeration consist in few lines of codes with the \textit{Type Safety} that \acrshort{hs} provides.

\subsection{Empirical Evaluation}\label{sec:new:eval}
The empirical study aims at evaluating $\dpwcc$ implemented \acrshort{dpfh} to verify that the performance is suitable compared with the empirical evaluation conducted on \autoref{prole}. 
Our goal is to answer the following research questions: 

\begin{inparaenum}[\bf {\bf RQ}1\upshape)]
\label{res:question}
    \item Does $\dpwcc$ in \acrshort{dpfh} exhibit similar execution time performance compared with \textit{Proof of Concept} evaluation?
    \item Does $\dpwcc$ in \acrshort{dpfh} exhibit continuous behavior enumerating connected components incrementally?
\end{inparaenum}

In order to answer the former questions, we have conducted the same experiments that we did on \textit{Proof of Concept} evaluation, but not focusing on the comparison with default \acrshort{hs} \texttt{containers} implementation. 
Another topic that we haven't measured in this empirical analysis is the performance of \textit{Memory and Threads} allocation. 
The networks selected for this evaluation are the same as the networks used on \autoref{prole}.

\subsubsection{Running Architecture}
All the experiments have been executed in a $x86$ $64$ bits architecture with a \textit{$6$-Core Intel Core i7} processor of $2,2$ GHz which can emulate up to $12$ virtual cores. This processor has \emph{hyper-threading} enable. Regarding memory, the machine has $32 GB$ \emph{DDR4} of RAM, $256\ KB$ of L2 cache memory, and $9\ MB$ of L3 cache.

\subsubsection{Haskell Setup}
Regarding specific libraries and compilations flags used on \acrshort{hs}, we have used \acrshort{ghc} version $8.10.4$. 
We have also used the following set of libraries: \mintinline{bash}{bytestring 0.10.12.0} \cite{bytestring}, \mintinline{bash}{containers 0.6.2.1} \cite{containers}, \mintinline{bash}{relude 1.0.0.1} \cite{relude} and \mintinline{bash}{dynamic-pipeline 0.3.2.0} \cite{dynamic-pipeline}. 
The use of \texttt{relude} library is because we disabled \mintinline{haskell}{Prelude} from the project with the language extension \mintinline{haskell}{NoImplicitPrelude} \cite{extensions}. 
Regarding compilation flags (\acrshort{ghc} options) we have compiled our program with \mintinline{bash}{-threaded}, \mintinline{bash}{-O3}, \mintinline{bash}{-rtsopts}, \mintinline{bash}{-with-rtsopts=-N}. 
Since we have used \texttt{stack} version $2.5.1$ \cite{stack} as a building tool on top of \acrshort{ghc} the compilation command is \mintinline{bash}{stack build}\footnote{For more information about package.yaml or cabal file please check https://github.com/jproyo/upc-miri-tfm/tree/main/connected-comp}.

\subsection{Experiments Definition}\label{sub:new:exp:def}
\paragraph{E1: Implementation Analysis}
Similar to what we have measure on \autoref{prole}, in this experiment we measure \acrshort{ghc} statistics running time enabling \mintinline{bash}{+RTS -s} flags.
The metrics that we measure are \emph{MUT Time} which is the amount of time in seconds \acrshort{ghc} is running computations and \emph{GC Time} which is the number of seconds that \acrshort{ghc} is running garbage collector. 
\emph{Total execution time} is the sum of both in seconds. This experiment will help to answer research question [RQ1].

\paragraph{E2: Benchmark Analysis}
This experiment measures \emph{Average Running Time}.
The \emph{Average Running Time} is the average running time of $1000$ resamples using \texttt{criterion} tool~\cite{criterion}. 
In each sample, the running time is measure from the beginning of the execution of the program until when the last answer is produced.
This experiment will help to answer research question [RQ1].

\paragraph{E3: Continuous Behavior - Diefficiency Metrics}
This experiment measures \acrlong{dm} described in \autoref{prem:dief}, and will help to answer research question [RQ2].

\subsection{Discussion of Observed Results}\label{new:experiments}
\subsubsection{Experiment: E1}\label{sub:new:sec:e1}
The following represents the \emph{Total execution time} for each of the networks using the new $\dpwcc$ algorithm, implemented with \acrshort{dpfh}.
 
\begin{table}[H]
  \centering
  \begin{tabular}{|l|r|r|r|r|}
   \hline
   \textbf{Network} & \textbf{Exec Param} & \textbf{MUT Time} & \textbf{GC Time} & \textbf{Total Time}\\
   \hline
   Enron Emails & \mintinline{bash}{+RTS -N4 -s} & 1.795s & 0.505s & 2.314s \\
   \hline
   Astro Physics Coll Net & \mintinline{bash}{+RTS -N4 -s} & 2.294s & 1.003s & 3.311s \\
   \hline
   Google Web Graph & \mintinline{bash}{+RTS -N8 -s} & 169.381s & 270.784s & 440.176s \\
   \hline
  \end{tabular}
 \caption{Total Execution times of each of the networks using $\dpwcc$ algorithm implemented with \acrshort{dpfh}. \textit{MUT Time} is the time of running or executing code and \textit{GC Time} is the time that the program spent doing Garbage collection. \textit{Total Execution time} is the sum of both times}
 \label{table:new:wcc:dpfh:5}
 \end{table}

In \autoref{table:new:wcc:dpfh:5}, we are obtaining similar execution times compared with \autoref{table:5} in \autoref{prole}.
In fact, for \textit{Enron Emails} and \textit{Astro Physics Coll} networks, all times are better than the implementation of the \textit{Proof of Concept}.
Regarding \textit{Google Web} network, the time is slighlity worse but there is the fact that \acrshort{dpfh} is adding some overhead over plain code execution.
According to this results we can partially answer [RQ1], because the implementation of $\dpwcc$ with \acrshort{dpfh} has similar performance compared with \textit{Proof of Concept} implementation. 

\subsubsection{Experiment: E2}\label{sub:new:exp:2}
In \autoref{fig:new:1}, we can appreciate the \textit{Average Execution Time} for each of the networks after running \texttt{criterion}~\cite{criterion} tool.

\begin{minipage}[t]{\linewidth}
  \includegraphics[width=\textwidth]{bench_1_new.png}
  \captionsetup{type=figure}
  \captionof{figure}{$\dpwcc$ implemented with \acrshort{dpfh}. Average Execution time of running $1000$ samples over each of the networks.}
  \label{fig:new:1}
\end{minipage}

If we compare those \textit{Average Execution Times} with the previous obtained in \autoref{prole}, we have the following comparison table.

 \begin{table}[H]
  \centering
  \begin{tabular}{|l|l|l|l|}
   \hline
   \textbf{Network} & \textbf{DPF-WCC} & \textbf{\acrshort{dpwcc}} & \textbf{Speed-up}\\
   \hline
   Enron Emails & 4.30s & 4.68s &  0.91\\
   \hline
   Astro Physics Coll Net & 4.76s  & 4.98s & 0.95\\
   \hline
   Google Web Graph & 456s & 386s & -1.18\\
   \hline
  \end{tabular}
 \caption{Comparison of Average Execution Times between $\dpwcc$ implemented with \acrshort{dpfh} and the implementation of Proof of Concept (\acrshort{dpwcc})}
 \label{table:new:6}
 \end{table}

As we can see in \autoref{table:new:6}, all the \textit{Average Execution times} are better for the new implementation with \acrshort{dpfh} compared with the \acrshort{dpwcc}.
As it is consistent with the \textit{Total Execution time} of the previous experiment in \autoref{sub:new:sec:e1}, the only networks that performs slightly worse is \textit{Google Web}, but the difference is not significant enough taking into consideration the overhead introduced by \acrshort{dpfh}.
These results allow for answering research question [RQ1] completely ensuring that the performance in terms of execution is better for smaller networks and almost similar for bigger networks. 

\subsubsection{Experiment: E3}\label{sub:new:sec:e2}
In the case of continuous behavior analysis, we have run the comparison against \acrshort{hs} \texttt{containers} again in order to visually appreciate the incremental generation of results. 

\begin{figure}[!htp]
  \centering
  \begin{subfigure}[t]{0.3\textwidth}
   \includegraphics[width=1\linewidth, height=0.2\textheight]{email_enron}
   \caption{email-Enron \acrlong{dm} Line Plot}
    \label{fig:dief:1}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.3\textwidth}
   \includegraphics[width=1\linewidth, height=0.2\textheight]{ca_astroph}
   \caption{ca-AstroPh \acrlong{dm} Line Plot}
    \label{fig:dief:2}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.3\textwidth}
   \includegraphics[width=1\linewidth, height=0.2\textheight]{web_google}
   \caption{web-Google \acrlong{dm} Line Plot}
    \label{fig:dief:3}
  \end{subfigure}\hfill
   \caption{These figures show \acrshort{dt} observed results after running all the scenarios for each network. $y$ axis represents the number of Answers produced and $x$ axis is the $t$ time of the \acrshort{dt} metric describe in \autoref{prem:dief}. The more data points distributed throughout the $x$ axis, the higher, the continuous behavior.}
   \label{fig:dief:all}
 \end{figure}

\begin{table}[htp!]
  \centering
  \begin{tabular}{|p{0.25\linewidth}|c|c|c|}
    \hline
   \textbf{Network} & \textbf{Algorithm} & \textbf{dief@t Metric}  & \textbf{dief@k Metric}\\
   \hline
   ca-AstroPh & $\dpwcc$ in \acrshort{dpfh} & $5.48 \times 10^2$ & $3.94 \times 10^2$\\
   \hline
   email-Enron & $\dpwcc$ in \acrshort{dpfh} & $4.35 \times 10^3$ & $4.02 \times 10^3$\\
   \hline
   web-Google & $\dpwcc$ in \acrshort{dpfh} & $3.48 \times 10^4$ & $3.48 \times 10^4$ \\
  \hline
  \end{tabular}
  \caption{This tables shows the \acrshort{dt} and \acrshort{dk} values gather for  $\dpwcc$ in \acrshort{dpfh}. We can appreciate that in all cases $\dpwcc$ has a higher value of \acrshort{dt} and a lower value of \acrshort{dk} showing continuos behavior}
 \label{table:e1:dm:values}
 \end{table}

 \begin{figure}[!htp]
  \centering
  \begin{subfigure}[t]{0.3\textwidth}
   \includegraphics[width=1\linewidth, height=0.2\textheight]{email_enron_radar}
   \caption{email-Enron \acrlong{dm} Radar Plot}
    \label{fig:dief:4}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.3\textwidth}
   \includegraphics[width=1\linewidth, height=0.2\textheight]{ca_astroph_radar}
   \caption{ca-AstroPh \acrlong{dm} Radar Plot}
    \label{fig:dief:5}
  \end{subfigure}\hfill
  \begin{subfigure}[t]{0.3\textwidth}
   \includegraphics[width=1\linewidth, height=0.2\textheight]{web_google_radar}
   \caption{web-Google \acrlong{dm} Radar Plot}
    \label{fig:dief:6}
  \end{subfigure}\hfill
   \caption{Radial plots show how the different dimensions values provided by \acrshort{dtkp} tool such as \acrshort{tt}, \acrshort{tfft}, \acrshort{dt}, \acrshort{et} and \acrshort{comp} are related each other for each experimental case. These figures show radial plot observed results after running for each network. \acrshort{dt} is described in \autoref{prem:dief}.}
   \label{fig:dief:radial:all}
 \end{figure}

Based on the results shown in \autoref{fig:dief:all}, all the solutions in \acrshort{dpwcc} show continuous behavior.
Moreover in \autoref{table:e1:dm:values}, $\dpwcc$ has a higher value of \acrshort{dt} and a lower value of \acrshort{dk} confirming the continuos behavior that we can see in the images.
As we can appreciate in \autoref{fig:dief:radial:all} radar plots our previous analysis can be confirmed.

In conclusion, we can say that regarding [RQ2] (\autoref{res:question}) although \acrshort{dpwcc} is faster than the traditional approach, the speed-up dimension execution factor is not always the most interest analysis that we can have, because as we have seen even when in the case of \emph{web-Google} Graph \acrshort{dpwcc} is slower at execution, it is at least generating incremental results without the need to wait for the rest of the computations.

\iffalse
\subsubsection{Experiment: E3}
For this type of analysis, our experiment focuses on \emph{email-Enron} network \cite{netenron} only because profiling data generated by \acrshort{ghc} is big enough to conduct the analysis and on the other, and enabling profiling penalize execution time.

\paragraph{Multithreading} For analyzing parallelization and multithreading we have used \textit{ThreadScope} \cite{threadscope} which allows us to see how the parallelization is taking place on \acrshort{ghc} at a fine grained level and how the threads are distributed throughout the different cores requested with the \mintinline{bash}{-N} execution \texttt{ghc-option} flag.

\begin{minipage}[t!]{\linewidth}

  \includegraphics[width=\textwidth]{screen_1}
  \captionsetup{type=figure}
  \captionof{figure}{Threadscope Image of General Execution}
  \label{fig:3}
\end{minipage}

In \autoref{fig:3}, we can see that the parallelization is being distributed evenly among the $4$ Cores that we have set for this execution.
The distribution of the load is more intensive at the end of the execution, where \mintinline{haskell}{actor2} filter stage 
%as it can be seen in \autoref{src:haskell:3}, 
of the algorithm is taking place and different filters are reaching execution of that second actor.

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
     \includegraphics[width=0.48\textwidth] {screen_2}
     %[width=10cm, height=9cm]
       \end{center}
     \caption{Threadscope Image of Zoomed Fraction}
     \label{fig:4}
 %\end{figure}
 \end{wrapfigure}
Another important aspect shown in \autoref{fig:3}, is that this work is not so significant for \acrshort{ghc} and the threads and distribution of the work keeps between 1 or 2 cores during the execution time of the \mintinline{haskell}{actor1}. However, the usages increase on the second actor as pointed out before. In this regard, we can answer research questions [Q1] and [Q3] (\autoref{res:question}), verifying that \acrshort{hs} not only supports the required parallelization level but is evenly distributed across the program execution too.

Finally, it can also be appreciated that there is no sequential execution on any part of the program because the $4$ cores have \textit{CPU} activity during the whole execution time. This is because as long the program start, and because of the nature of the \acrshort{dp} model, it is spawning the \textit{Source} stage in a separated thread. This is a clear advantage for the model and the processing of the data since the program does not need to wait to do some sequential processing like reading a file, before start computing the rest of the stages.




%\begin{minipage}[t!]{\linewidth}
%\begin{center}
%  \includegraphics[width=0.6\textwidth]{screen_2}
%  \captionsetup{type=figure}
%  \captionof{figure}{Threadscope Image of Zoomed Fraction}
%  \label{fig:4}
%  \end{center}
%\end{minipage}

\autoref{fig:4} zooms in on \textit{ThreadScope} output in a particular moment, approximately in the middle of the execution. We can appreciate how many threads are being spawned and by the tool and if they are evenly distributed among cores. The numbers inside green bars represent the number of threads that are being executed on that particular core (horizontal line) at that execution slot. Thus, the number of threads varies among slot execution times because as it is already known, \acrshort{ghc} implements \emph{Preemptive Scheduling} \cite{lightweightghc}.

Having said that, it can be appreciated in \autoref{fig:4} our first assumption that the load is evenly distributed because the mean number of executing threads per core is $571$.

\paragraph{Memory allocation} Another important aspect in our case is how the memory is being managed to avoid memory leaks or other non-desired behavior that increases memory allocation during the execution time. This is even more important in the particular implementation of \acrshort{wcc} using \acrshort{dp} model because it requires to maintain the set of connected components in memory throughout the execution of the program or at least until we can output the calculated \acrshort{wcc} if we reach to the last \textit{Filter} and we know that this \acrshort{wcc} cannot be enlarged anymore. 

In order to verify this, we measure memory allocation with \textit{eventlog2html} \cite{eventlog2html} which converts generated profiling memory eventlog files into graphical HTML representation. 

\begin{wrapfigure}{r}{0.5\textwidth}
  \begin{center}
     \includegraphics[width=0.5\textwidth] {visualization}
     %[width=10cm, height=9cm]
       \end{center}
     \caption{Memory Allocation}
     \label{fig:5}
 %\end{figure}
 \end{wrapfigure}
 
%\begin{minipage}[t]{\linewidth}
%\begin{center}
%  \includegraphics[width=0.6\textwidth]{visualization}
%  \captionsetup{type=figure}
%  \captionof{figure}{Memory Allocation}
%  \label{fig:5}
%  \end{center}
%\end{minipage}

As we can see in \autoref{fig:5}, \acrshort{dpwcc} does an efficient work on allocating memory since we are not using more than $57$ MB of memory during the whole execution of the program.

On the other hand, if we analyze how the memory is allocated during the execution of the program, it can also be appreciated that most of the memory is allocated at the beginning of the program and steadily decrease over time with a small peak at the end that does not overpass even half of the initial peak of $57$ MB. The explanation for this behavior is quite straightforward because at the beginning we are reading from the file and transforming a \mintinline{haskell}{ByteString} buffer to \mintinline{haskell}{(Int, Int)} edges. This is seen in the image in which the dark blue that is on top of the area is \mintinline{haskell}{ByteString} allocation. Light blue is allocation of \mintinline{haskell}{Maybe a} type which is the type that is returned by the \textit{Channels} because it can contain a value or not. Data value \mintinline{haskell}{Nothing} is indicating end of the \textit{Channel}. 
%as we can see in \autoref{src:haskell:f3}.

Another important aspect is the green area which represents \mintinline{haskell}{IntSet} allocation, which in the case of our program is the data structure that we use to gather the set of vertices that represents a \acrshort{wcc}. This means that the amount of memory used for gathering the \acrshort{wcc} itself is minimum and it is decreasing over time, which is another empirical indication that we are incrementally releasing results to the user. It can be seen as well that as long the green area reduces the lighter blue (\mintinline{haskell}{MUT_ARR_PTRS_CLEAN} \cite{ghcheap}) increases at the same time indicating that the computations for the output (releasing results) is taking place. 

Finally, according to what we have stated above, we can answer the question [Q3] (\autoref{res:question}) showing that not only memory management was efficient, but at the same time, the memory was not leaking or increasing across the running execution program.
\fi