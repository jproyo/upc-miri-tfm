\section{Related Work}\label{section:related-work}

\paragraph{Streaming in Haskell Language}
Streaming computational models have been implemented in \acrlong{hs} during the last $10$ years. One of the first libraries in the ecosystem was \mintinline{shell}{conduit}\footnote{\url{https://hackage.haskell.org/package/conduit}} in 2011.
After that, several efforts on improving streaming processing on the language has been made not only at abstraction level for the user but as well as performance execution 
improvements like \mintinline{shell}{pipes}~\footnote{\url{https://hackage.haskell.org/package/pipes}} and \mintinline{shell}{streamly}\footnote{\url{https://hackage.haskell.org/package/streamly}} lately.
Moreover, there is an empirical comparison between those three, where a benchmark analysis has been conducted\footnote{\url{https://github.com/composewell/streaming-benchmarks}}.

Although most of those libraries offer the ability to implement \acrshort{dap} and \acrshort{pip}, none of them provide clear abstractions to create \acrshort{dp} models because
the setup of the stages should be provided beforehand. In the context of this work, we have done a proof of concept at the beginning, 
but it was not possible to adapt any of those libraries to implement properly \acrshort{dp}. 
The closest we have been to implement \acrshort{dp} with some of those libraries was when we explored \mintinline{shell}{streamly}.
In this case, there is a \mintinline{haskell}{foldrS} combinator that could have been proper for the purpose of generating a dynamic pipeline of stages based on the data flow. However, it was not possible to manipulate the channels between the stages to control the flow of the data. It is important to remark that, even though, the  library  \mintinline{shell}{streamly} implements channels, they are hidden from the end-user, and there is not a  clear way to manipulate them.

To the best of our knowledge, no similar library under the  \acrshort{dp} approach has been written in \acrlong{hs}. 
One important motivation to develop our own framework is that  we not only  want to satisfy our research needs but, as a novel contribution, we aim at providing a \acrshort{dpf} to the \acrshort{hs} community as well. We hope this contribution encourages and helps writing algorithms under the Dynamic Pipeline Paradigm. 
\iffalse
Several implementations for streaming processing models \footnote{\url{https://hackage.haskell.org/package/conduit}}\footnote{\url{https://hackage.haskell.org/package/pipes}}\footnote{\url{https://hackage.haskell.org/package/streamly}} in \acrshort{hs} have arisen over the years. All these libraries have their abstractions and can do data streaming processing in a fast way with different performance according to recent benchmarks\footnote{\url{https://github.com/composewell/streaming-benchmarks}}. Although they seem to be suitable for implementing a $\DP$, it is required to know pipeline stages disposition beforehand, and it is hard to achieve a succinct and expressive implementation of a \acrshort{dpf}. Moreover, since they have been conceived as a data parallel streaming model \cite{hr19} by design instead of pipeline parallel streaming, implementing $\DP$ using these tools becomes counter-intuitive and hard to achieve.
\fi
Another kind of streaming implementation in \acrshort{hs} is described in \cite{parallelbook}. In that work, the author describes how to encode pipeline parallelism with \mintinline{haskell}{Par} \textit{Monad}. Although this could have been a suitable alternative for implementing $\DP$, the parallelization level used by \mintinline{haskell}{Par} \textit{Monad} is sparks \cite{sparks}. As we have explained in section \autoref{section:prob:dp:haskell}, we do not require to reach that level of parallelization in our current model.

In regards to other $\DP$ language implementations, a significant contribution on \cite{dpp_triangles} has been done, where a $\DP$ implementation in \acrfull{go} for counting triangles of graphs is compared against MapReduce. Those experiment results have shown how $\DP$ in \acrshort{go} improves the performance in terms of execution time and memory depending on the graph topology. It would be interesting and a matter of future work, to compare different language implementations of $\DP s$, taking into consideration those promising results and the ones presented in this article.

\iffalse
In particular, the problem presented in \autoref{sub:sec:mot:ex} is one of the algorithms in which the amount of stages that could run in parallel is the worst case having one Stage per edge at most, but still in that scenario the number of threads can be efficiently handled by \acrshort{ghc}. Therefore, there is no need for such a fine grained parallelization level as it could be required when the data should be split into the smallest processing units as possible. 
\fi
