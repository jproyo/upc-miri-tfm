\section{Related Work}\label{section:related-work}

\paragraph{Streaming Processing}
The development of streaming processing techniques have potentiated areas of massive data processing for data mining algorithms, big data analysis, IoT applications, etc.  \acrfull{ds} has been studied using different approaches (\cite{SA-libro,exploiting, onthefly} and see~\cite{hr19} for a survey) allowing to process a large amount of data efficiently with an intensive level of parallelization. There are two main different parallelization streaming computational models: \acrfull{dap} and \acrfull{pip} (see \autoref{prelim-pip}). According to the \acrshort{dap} approach, data are separated and processed in parallel and, all  computations taking place in parallel over the different subsets of data are independent of each other. 
A common model that has been proved successful over the last decade is \acrfull{mr}~\cite{mapreduce}. Different frameworks or tools like Hadoop~\footnote{\url{https://hadoop.apache.org/}}, Spark~\footnote{\url{https://spark.apache.org/}}, etc., support this computational model efficiently. One of the main advantages of this kind of model is the ability to implement stateless operators~\cite{hr19}. Data are treated in different threads or processors without the need for contextual information. However, when it is necessary to be aware of the context, parallelization is penalized, each computational step should be fully calculated before proceeding with the others. This is the case of the  \mintinline{shell}{reduce} operation on many frameworks or tools. This problem makes the \acrshort{dap} approach non-viable for delivering results incrementally. Contrary, the dynamic pipeline paradigm exploits pipeline parallelism, allowing for enlarging and shrinking pipeline stages or pipes dynamically. Moreover, each stage produces results as soon as they are ready, enabling, thus, the continuous generations of answers.  

\paragraph{Streaming in Haskell Language}
Streaming computational models have been implemented in \acrlong{hs} during the last ten years. One of the first libraries in the ecosystem was \mintinline{shell}{conduit}\footnote{\url{https://hackage.haskell.org/package/conduit}} in 2011.
After that, several efforts on improving streaming processing on the language has been made not only at abstraction level for the user but as well as performance execution improvements like \mintinline{shell}{pipes}~\footnote{\url{https://hackage.haskell.org/package/pipes}} and \mintinline{shell}{streamly}\footnote{\url{https://hackage.haskell.org/package/streamly}} lately.
Although most of those libraries offer the ability to implement \acrshort{dap} and \acrshort{pip}, none of them provide clear abstractions to create \acrshort{dp} models because the setup of the stages should be provided beforehand. In the context of this work, we have done a proof of concept at the beginning, but it was not possible to adapt any of those libraries to implement properly \acrshort{dp}.  \mintinline{shell}{streamly} looks as the most promising library to implement \acrshort{dp}. It provides a \mintinline{haskell}{foldrS} combinator that seems to be proper for generating a dynamic pipeline of stages based on the data flow. However, it was not possible to manipulate the channels between the stages to control the data flow. Moreover,  even though the library  \mintinline{shell}{streamly} implements channels, they are hidden from the end-user, and there is not a  clear way to manipulate them. 
To the best of our knowledge, no similar library under the  \acrshort{dp} approach has been written in \acrlong{hs}. 
One crucial motivation to develop our Dynamic Pipeline framework is that we not only want to satisfy our research needs but, as a novel contribution, we aim at providing a \acrshort{dpf} to the \acrshort{hs} community as well. We hope this contribution encourages and helps writing algorithms under the Dynamic Pipeline Paradigm. 
\iffalse
Several implementations for streaming processing models \footnote{\url{https://hackage.haskell.org/package/conduit}}\footnote{\url{https://hackage.haskell.org/package/pipes}}\footnote{\url{https://hackage.haskell.org/package/streamly}} in \acrshort{hs} have arisen over the years. All these libraries have their abstractions and can do data streaming processing in a fast way with different performance according to recent benchmarks\footnote{\url{https://github.com/composewell/streaming-benchmarks}}. Although they seem to be suitable for implementing a $\DP$, it is required to know pipeline stages disposition beforehand, and it is hard to achieve a succinct and expressive implementation of a \acrshort{dpf}. Moreover, since they have been conceived as a data parallel streaming model \cite{hr19} by design instead of pipeline parallel streaming, implementing $\DP$ using these tools becomes counter-intuitive and hard to achieve.
\fi
Another kind of streaming implementation in \acrshort{hs} is described in \cite{parallelbook}. In that work, the author describes how to encode pipeline parallelism with \mintinline{haskell}{Par} \textit{Monad}. 
Although this could have been a suitable alternative for implementing $\DP$, the parallelization level used by \mintinline{haskell}{Par} \textit{Monad} is sparks \cite{sparks}. We do not require reaching that level of parallelization in DDF. In regard to other $\DP$ language implementations, a significant contribution to \cite{dpp_triangles} has been done, where a $\DP$ implementation in \acrfull{go} for counting triangles of graphs is compared against MapReduce. The reported experiment results show how $\DP$ in \acrshort{go} improves the performance in terms of execution time and memory depending on the graph topology. A comparison of different streaming implementations of $\DP$ represents a valuable assessment of the power of the paradigm. Nevertheless, this study is out of the scope of this paper, and it is part of our future work.   

\paragraph{Streaming frameworks} 
Regarding performance, in general, the primary metrics considered when evaluating stream processing pipelines with massive input data are latency, throughput, and resource utilization \cite{van2020evaluation}. Latency indicates how long the framework takes to deliver a result. Throughput captures the quantity of data processed within a unit of time. A good pipeline framework behavior must report low latency and high throughput. However, there are stream processing problems where obtaining results incrementally is a critical issue, as we said before. This is the case, for instance, of biological, medical, or social systems that need to identify patterns/relationships to make real-time decisions.
Consequently, measuring only latency and throughput of stream processing frameworks will not illustrate the goodness of a framework when continuous performance is required. In this regard, diefficiency metrics proposed in \cite{diefpaper} are proper tools for this kind of assessment. Regarding resource utilization metrics, one of the significant challenges when deploying a  stream processing system on top of a pipeline framework is to estimate the resources consumed during all the time the system is executing. There could be peaks of consumption, but a resource over-estimation means a loss of money and possibly an overhead of the system itself since the over-administration of the resources.   According to the \acrshort{dp} approach supported by the \acrshort{dpf}, this framework is an elastic parallel stream processing framework. It offers users the possibility of adopting a \textit{pay-as-you-go} policy model \cite{payg} use of resources and, hence,  the chance of saving money which is a critical issue for any business. Therefore, the resource utilization metric \textit{per se} is not representative enough in front of a stream processing system with peaks and valleys of resource consumption.

