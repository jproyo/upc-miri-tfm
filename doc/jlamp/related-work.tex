\section{Related Work}\label{section:related-work}
Several implementations for streaming processing models \cite{conduit, pipes, streamly} in \acrshort{hs} have arisen over the years. All these libraries have their abstractions and can do data streaming processing in a fast way with different performance according to recent benchmarks \cite{benchstreamhs}. Although they seem to be suitable for implementing a $\DP$, it is required to know pipeline stages disposition beforehand, and it is hard to achieve a succinct and expressive implementation of a \acrshort{dpf}. Moreover, since they have been conceived as a data parallel streaming model \cite{hr19} by design instead of pipeline parallel streaming, implementing $\DP$ using these tools becomes counter-intuitive and hard to achieve.

Another kind of streaming implementation in \acrshort{hs} is described in \cite{parallelbook}. In that work, the author describes how to encode pipeline parallelism with \mintinline{haskell}{Par} \textit{Monad}. Although this could have been a suitable alternative for implementing $\DP$, the parallelization level used by \mintinline{haskell}{Par} \textit{Monad} is sparks \cite{sparks}. As we have explained in section \autoref{section:prob:dp:haskell}, we do not require to reach that level of parallelization in our current model.

In regards to other $\DP$ language implementations, a significant contribution on \cite{dpp_triangles} has been done, where a $\DP$ implementation in \acrfull{go} for counting triangles of graphs is compared against MapReduce. Those experiment results have shown how $\DP$ in \acrshort{go} improves the performance in terms of execution time and memory depending on the graph topology. It would be interesting and a matter of future work, to compare different language implementations of $\DP s$, taking into consideration those promising results and the ones presented in this article.

\iffalse
In particular, the problem presented in \autoref{sub:sec:mot:ex} is one of the algorithms in which the amount of stages that could run in parallel is the worst case having one Stage per edge at most, but still in that scenario the number of threads can be efficiently handled by \acrshort{ghc}. Therefore, there is no need for such a fine grained parallelization level as it could be required when the data should be split into the smallest processing units as possible. 
\fi
